# AUTHOR: UTKARSH MISHRA
"""This program is used to analyze the transcript generated by the analysis.py program and generate
the action items present in the given conversation and assign them accordingly to the person reponsible"""
"""To use this file independently the transcript of the conversation with tags must be
present in 'conv_with_tags.txt' file and the read, write persmissions are required to read the 'conv_with_tags.txt' file
and write the results in the 'action.txt' file."""
import sys
import email
import os
from os import path
import re
import glob
import _pickle as pickle
import spacy
from nltk import sent_tokenize
from nltk.tokenize import PunktSentenceTokenizer


sent_tokenizer = PunktSentenceTokenizer()
nlp = spacy.load('en_core_web_sm')

#Use the below lines if this file is used independently
"""
action_file = 'action.txt'
"""

action_file = 'action.txt'

"""This function removes the punctuation marks from the given transcript and returns the processed
clean text which is then fed to the model in the following function to give the resulting output.
The regex expression can be modified to fit the type of transcript generated.
"""

def clean_text(text):
    filt = re.compile("[^A-Za-z0-9-'?.,:]+")
    t = filt.sub(' ', str(text))
    sent = " ".join(t.split())
    return sent

"""This function takes in two parameters viz. the file in which the conversation transcript is present and 
the model which will be used to predict the type of sentence present in the conversation. It predicts the type
of sentence i.e. action sentence or not, using the specified model and performs subject extraction on the 
action sentences. The subject and the action item is then stored locally in the file 'action.txt' and then
subsequently mailed to the required parties.
"""

def process(filename, model):
    vect, cls = model

    final_sents = []
    final_sents_with_tags = []
    subjects = []
    action_items = []
    tags = []
    sentences = []
    
    
    with open(filename) as h:
        data = h.read()
        data = data.splitlines()        
        for i in range(len(data)):
            sep = data[i].split(':')
            tags.append(sep[0])
            sentences.append(sep[1])
        text = clean_text(sentences)
        sents = sent_tokenizer.tokenize(text)

    
    for i in range(len(sents)):
        final_sents.append(sents[i])

    X = vect.transform(final_sents)
    Y = cls.predict(X)


    for sent, y in zip(final_sents, Y):
        if(y):
            doc = nlp(sent)
            sub_toks = [tok for tok in doc if (tok.dep_ == "nsubj") ]
            you = [i for i in sub_toks if str(i) == 'you']
            if(len(you) >= 1):
                final_sents_with_tags.append([you, sent])
            action_items.append([[(X.text, X.label_) for X in doc.ents], str(sent)])

    
    for item in action_items:
        try:
            if(item[0][0][1] != 'PLACE_HOLDER'):
                subjects.append(item[0][0][0])

        except:
            pass

    
    cleaned_text = []
    k = 0
    for i in range(len(tags)):
        clean_sent = clean_text(sentences[i])
        try:
            if(str(action_items[k][1])[5:] == str(clean_sent)):
                k += 1
                cleaned_text.append([str(clean_sent), tags[i+2]])
        except:
            pass
    
    
    tasks = []
    for clean in cleaned_text:
        for subject in subjects:
            if(subject in clean[0]):
                tasks.append([subject, clean])
    with open('static/results/'+action_file, 'w') as a:
        pass
    for task in tasks:
        with open('static/results/'+action_file, 'a+') as actions:
            string = str(task[0]) + ': ' + str(task[1][0]) +'\n'
            actions.write(string)


"""This program uses a LSTM model to predict the type of sentence that is present in the given transcript.
The model used can be changed by specifying the model path in the following function. The output of the model
used is in binary format i.e. either it is a action sentence or it isn't. If another model is used modify the 
process, function present above, accordingly.
"""


def filter(conv_file):

    filename = conv_file

    with open('model.pkl', 'rb') as h:
        model = pickle.load(h)

    process(filename, model)

    return action_file